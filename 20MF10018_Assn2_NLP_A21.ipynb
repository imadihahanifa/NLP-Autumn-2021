{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20MF10018_Assn2_NLP_A21.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zyJ25uz0kSaw"},"source":["# **Assignment-2 for CS60075: Natural Language Processing**\n","\n","#### Instructor : Prof. Sudeshna Sarkar\n","\n","#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n","\n","#### Date of Announcement: 15th Sept, 2021\n","#### Deadline for Submission: 11.59pm on Wednesday, 22nd Sept, 2021 \n","#### Submit this .ipynb file, named as `<Your_Roll_Number>_Assn2_NLP_A21.ipynb`"]},{"cell_type":"markdown","metadata":{"id":"Ao1nhg9RknmF"},"source":["The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.  This dataset consists of 50k movie reviews (25k positive, 25k negative). You can download the dataset from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ONM5Q4SCe9Mr"},"source":["Please submit with outputs. "]},{"cell_type":"markdown","metadata":{"id":"c_NUYXssNgqT"},"source":[""]},{"cell_type":"code","metadata":{"id":"ElRkQElWUMjG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632425330563,"user_tz":-330,"elapsed":4857,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"ba53481a-0b51-4b0b-b477-4c6a750b58cc"},"source":["import re\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import keras\n","from sklearn.metrics import classification_report , accuracy_score\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from collections import Counter, defaultdict\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stopwords = stopwords.words('english')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"fhHRim2AUm4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632425396133,"user_tz":-330,"elapsed":1374,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"f3baa092-c9b1-4a0a-8d36-d452cf82086d"},"source":["#Load the IMDB dataset. You can load it using pandas as dataframe\n","dataset = pd.read_csv('/content/IMDB Dataset.csv.zip')\n","print(dataset)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  review sentiment\n","0      One of the other reviewers has mentioned that ...  positive\n","1      A wonderful little production. <br /><br />The...  positive\n","2      I thought this was a wonderful way to spend ti...  positive\n","3      Basically there's a family where a little boy ...  negative\n","4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n","...                                                  ...       ...\n","49995  I thought this movie did a down right good job...  positive\n","49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n","49997  I am a Catholic taught in parochial elementary...  negative\n","49998  I'm going to have to disagree with the previou...  negative\n","49999  No one expects the Star Trek movies to be high...  negative\n","\n","[50000 rows x 2 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"lK_Hn2f6VMP7"},"source":["# Preprocessing\n","PrePrecessing that needs to be done on lower cased corpus\n","\n","1. Remove html tags\n","2. Remove URLS\n","3. Remove non alphanumeric character\n","4. Remove Stopwords\n","5. Perform stemming and lemmatization\n","\n","You can use regex from re. "]},{"cell_type":"code","metadata":{"id":"5B5lHZPsVOXv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632425398213,"user_tz":-330,"elapsed":11,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"e296160b-6ca1-4314-9b94-a0cc63eb819f"},"source":["#lower case the corpus; remove- html tags, urls, non alpha numerics, stopwords ;stemming&lemmatization\n","def preprocess(raw_text):   \n","  cleantext = raw_text.lower()\n","  cleaner = re.compile('<.*?>')\n","  cleantext = re.sub(cleaner, ' ', cleantext)    \n","  cleantext = re.sub(r'http\\S+', '', cleantext)  \n","  cleantext = re.sub(r'[^\\w\\s]', '', cleantext)  \n","  lemmatizer = WordNetLemmatizer()  \n","  cleanwords = [lemmatizer.lemmatize(word) for word in word_tokenize(cleantext) if word not in stopwords]  \n","  cleantext = ' '.join(cleanwords)  \n","  return cleantext\n","# preprocess the entire dataset\n","  def ppDataset(dataset):   \n","   for i in range(len(dataset)):\n","     dataset.iloc[i][0] = preprocess(dataset.iloc[i][0])\n","   return dataset\n","\n","  dataset = ppDataset(dataset)\n","print(\"Preprocessed dataset:\\n\", dataset)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed dataset:\n","                                                   review sentiment\n","0      One of the other reviewers has mentioned that ...  positive\n","1      A wonderful little production. <br /><br />The...  positive\n","2      I thought this was a wonderful way to spend ti...  positive\n","3      Basically there's a family where a little boy ...  negative\n","4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n","...                                                  ...       ...\n","49995  I thought this movie did a down right good job...  positive\n","49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n","49997  I am a Catholic taught in parochial elementary...  negative\n","49998  I'm going to have to disagree with the previou...  negative\n","49999  No one expects the Star Trek movies to be high...  negative\n","\n","[50000 rows x 2 columns]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oauoOiWduwYf","executionInfo":{"status":"ok","timestamp":1632425498399,"user_tz":-330,"elapsed":96831,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"86a66048-b8dc-417c-e367-42c1b74b92f8"},"source":["# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n","positive = 0\n","sumlen = 0\n","for i in range(len(dataset)):\n","  if dataset.iloc[i][1] == 'positive':\n","    positive += 1\n","  sumlen += len(word_tokenize(dataset.iloc[i][0]))\n","print(\"Average length of sentence = {: .4f} words\".format(sumlen/len(dataset)))\n","print(\"Proposition of data w.r.t class labels:\")\n","print(\"Positive reviews:{:4f}\".format(positive))\n","print(\"Negative reviews:{:.4f}\".format( len(dataset)- positive))\n","print(\"Proportion of postitive to negative reviews:{:.4}\".format(positive/ len(dataset)*100))\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Average length of sentence =  279.1375 words\n","Proposition of data w.r.t class labels:\n","Positive reviews:25000.000000\n","Negative reviews:25000.0000\n","Proportion of postitive to negative reviews:50.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"_FkJ-e2pUwun"},"source":["# Naive Bayes classifier"]},{"cell_type":"code","metadata":{"id":"eVq-mN28U_J4","executionInfo":{"status":"ok","timestamp":1632425498400,"user_tz":-330,"elapsed":6,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["# get reviews column from df\n","reviews = dataset['review'].values\n","\n","# get labels column from df\n","labels = dataset['sentiment'].values"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ljo5NquhXTXr","executionInfo":{"status":"ok","timestamp":1632411416148,"user_tz":-330,"elapsed":6,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["# Use label encoder to encode labels. Convert to 0/1\n","encoder = LabelEncoder()\n","encoded_labels = encoder.fit_transform(labels)\n","dataset['encoded'] = encoded_labels\n","encoder_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n","labels = dataset['encoded']\n","\n","# print(enc.classes_)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzG-C_EVWWET","executionInfo":{"status":"ok","timestamp":1632411419230,"user_tz":-330,"elapsed":348,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["# Split the data into train and test (80% - 20%). \n","# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n","train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, labels, test_size = 0.2, stratify = labels)\n","# train_sentences, test_sentences, train_labels, test_labels"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bz1YdsSkiWCX"},"source":["Here there are two approaches possible for building vocabulary for the naive Bayes.\n","1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n","2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n"," \n","You are supposed to go by the 2nd approach.\n"," \n","Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n","\n","> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n","\n","\n","$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n","\n","$N_{w_j}$ : Total count of features in class $w_j$\n","\n","$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n","\n","$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"1cllNfGmUr77","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632411431176,"user_tz":-330,"elapsed":8097,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"d54c0eca-583e-46b8-f3a9-976c37b3103a"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# Use Count vectorizer to get frequency of the words\n","'''\n","max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n","vec = CountVectorizer(max_features = 3000)\n","X = vec.fit_transform(Sentence_list)\n","'''\n","\n","\n","vec = CountVectorizer(max_features = 3000)\n","X = vec.fit_transform(train_sentences)\n","counts = X.sum(axis = 0).A1\n","vocab = list(vec.get_feature_names())\n","\n","freq = Counter(dict(zip(vocab, counts)))\n","\n","print(\"The 100 most common words in the reviews are: \", freq.most_common(100), sep = '\\n')\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The 100 most common words in the reviews are: \n","[('the', 534159), ('and', 259551), ('of', 231719), ('to', 214490), ('is', 169312), ('br', 162011), ('it', 152666), ('in', 149483), ('this', 120930), ('that', 114990), ('was', 76379), ('as', 73331), ('movie', 70623), ('for', 69914), ('with', 69723), ('but', 66832), ('film', 63768), ('you', 55454), ('on', 54453), ('not', 48806), ('he', 46890), ('are', 46641), ('his', 46014), ('have', 44152), ('one', 42942), ('be', 42608), ('all', 37549), ('at', 37372), ('they', 36359), ('by', 35412), ('an', 34306), ('who', 33687), ('so', 32640), ('from', 32248), ('like', 32049), ('there', 30185), ('or', 28674), ('just', 28041), ('her', 27915), ('out', 27451), ('about', 27309), ('if', 27186), ('has', 26474), ('what', 25698), ('some', 24855), ('good', 23839), ('can', 23376), ('more', 22411), ('when', 22334), ('very', 22247), ('she', 21778), ('up', 21122), ('no', 20296), ('time', 19976), ('my', 19963), ('even', 19862), ('would', 19661), ('which', 18729), ('only', 18589), ('see', 18503), ('really', 18417), ('story', 18411), ('their', 18229), ('had', 17581), ('me', 17088), ('we', 17052), ('well', 16965), ('were', 16910), ('than', 15417), ('much', 15397), ('get', 14898), ('bad', 14844), ('been', 14632), ('other', 14619), ('do', 14525), ('great', 14522), ('will', 14514), ('people', 14507), ('also', 14329), ('into', 14299), ('how', 14206), ('don', 14139), ('first', 14119), ('because', 14028), ('him', 14018), ('most', 13918), ('its', 12949), ('made', 12922), ('them', 12814), ('then', 12768), ('make', 12648), ('way', 12612), ('could', 12379), ('too', 12312), ('movies', 12201), ('any', 12030), ('after', 12005), ('characters', 11501), ('think', 11435), ('watch', 11212)]\n"]}]},{"cell_type":"code","metadata":{"id":"qzRvPjWaWUnm","executionInfo":{"status":"ok","timestamp":1632411431176,"user_tz":-330,"elapsed":7,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["# Use laplace smoothing for words in test set not present in vocab of train set\n","# Build the model. Don't use the model from sklearn\n","\n","class Naive_Bayes:\n","    def __init__(self, classes):\n","      self.classes = classes\n","\n","    def smoothing(self, word, tclass):          \n","      num = self.wcounts[tclass][word] + 1\n","      den = self.n_items[tclass] + len(self.vocab)\n","      return math.log(num / den)\n","\n","    def fit(self, X, y):\n","        self.vocab = vocab\n","        self.wcounts = {}\n","        self.n_items = {}\n","        self.log_p = {}\n","        n = len(X)\n","        grouped = self.group(X, y)\n","        for c, data in grouped.items():\n","          self.n_items[c] = len(data)\n","          self.log_p[c] = math.log(self.n_items[c] / n) \n","          self.wcounts[c] = defaultdict(lambda: 0)\n","          for txt in data:\n","            counts = Counter(nltk.word_tokenize(txt))\n","            for word, count in counts.items():\n","                self.wcounts[c][word] += count\n","        return self\n","\n","    def predict(self, X):\n","        result = []\n","        for text in X:\n","          scores = {c: self.log_p[c] for c in self.classes}\n","          words = set(nltk.word_tokenize(text))\n","          for word in words:\n","              if word not in self.vocab: \n","                continue\n","              for c in self.classes:\n","                log_wgc = self.smoothing(word, c)\n","                scores[c] += log_wgc\n","          result.append(max(scores, key = scores.get))\n","        return result\n","\n","    def group(self, X, y):\n","      data = {}\n","      for c in self.classes:                          \n","        data[c] = X[np.where(y == c)]\n","      return data"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"iE7pxWIYW1z0","executionInfo":{"status":"ok","timestamp":1632411437107,"user_tz":-330,"elapsed":3,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtQSl1zvW4DD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632411593241,"user_tz":-330,"elapsed":154779,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"f380d55a-382f-40ab-c442-6ca11370bc93"},"source":["# Test the model on test set and report Accuracy\n","import math\n","nb = Naive_Bayes(classes = np.unique(labels)).fit(train_sentences, train_labels)\n","\n","# Test the model on test set and report Accuracy\n","predicted_labels = nb.predict(test_sentences)\n","print(\"The accuracy of the Naive Bayes classifier is: \\n{:.4f}%\\n\\n\".format(accuracy_score(test_labels, predicted_labels) * 100))\n","print(\" \")\n","print(\"The classification report is as follows: \\n\\n\", classification_report(test_labels, predicted_labels))\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["The accuracy of the Naive Bayes classifier is: \n","83.9400%\n","\n","\n"," \n","The classification report is as follows: \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.80      0.90      0.85      5000\n","           1       0.89      0.77      0.83      5000\n","\n","    accuracy                           0.84     10000\n","   macro avg       0.85      0.84      0.84     10000\n","weighted avg       0.85      0.84      0.84     10000\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"WlNql0acU7sa"},"source":["# *LSTM* based Classifier\n","\n","Use the above train and test splits."]},{"cell_type":"code","metadata":{"id":"SkqnvbUOXoN0","executionInfo":{"status":"ok","timestamp":1632411593865,"user_tz":-330,"elapsed":628,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["# Hyperparameters of the model\n","tokenizer = Tokenizer()\n","word_index = tokenizer.word_index\n","vocab_size = len(tokenizer.word_index) + 1  # choose based on statistics\n","oov_tok = '<OOK>'\n","embedding_dim = 100\n","max_length = 150 # choose based on statistics, for example 150 to 200\n","padding_type='post'\n","trunc_type='post'"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeycEg9nZAOF","executionInfo":{"status":"ok","timestamp":1632411745730,"user_tz":-330,"elapsed":18822,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}}},"source":["# tokenize sentences\n","tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n","tokenizer.fit_on_texts(train_sentences)\n","word_index = tokenizer.word_index\n","\n","# convert train dataset to sequence and pad sequences\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n","\n","# convert Test dataset to sequence and pad sequences\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mtw3w895ZP39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632411747385,"user_tz":-330,"elapsed":1670,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"7bb78b87-c4ca-4467-cbab-ec37a7cb175f"},"source":["# model initialization\n","model = keras.Sequential([\n","    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n","    keras.layers.Dense(24, activation='relu'),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# compile model\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# model summary\n","model.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 150, 100)          100       \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 128)               84480     \n","_________________________________________________________________\n","dense (Dense)                (None, 24)                3096      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 25        \n","=================================================================\n","Total params: 87,701\n","Trainable params: 87,701\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":502},"id":"-KN6MONjassx","executionInfo":{"status":"error","timestamp":1632413347380,"user_tz":-330,"elapsed":632,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"7b586ac1-d92f-4108-efd5-318980da2b66"},"source":["#training the model\n","num_epochs = 5\n","history = model.fit(train_padded, train_labels, \n","                    epochs=num_epochs, verbose=1, \n","                    validation_split=0.1)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e9892bfe32ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m history = model.fit(train_padded, train_labels, \n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_split=0.1)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[28,0] = 1 is not in [0, 1)\n\t [[node sequential/embedding/embedding_lookup (defined at <ipython-input-14-edde0f88d9cb>:2) ]] [Op:__inference_train_function_4892]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/embedding/embedding_lookup:\n sequential/embedding/embedding_lookup/3073 (defined at /usr/lib/python3.7/contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"]}]},{"cell_type":"code","metadata":{"id":"TjEhWEr5Zq7M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631813455980,"user_tz":-330,"elapsed":13077,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"71be4ff1-43a0-4a42-d332-0922a46eed1b"},"source":["# Calculate accuracy on Test data\n","'''\n","prediction = model.predict(test_padded)\n","\n","'''\n","prediction = model.predict(test_padded)\n","# Get probabilities\n","print(\"Probabilities: \", prediction, sep='\\n')\n","\n","# Get labels based on probability 1 if p>= 0.5 else 0\n","for p in prediction:\n","  if p[0] >= 0.5:\n","    p[0] = 1\n","  else:\n","    p[0] = 0\n","prediction = prediction.astype('int32') \n","print(\"\\nLabels:\", prediction, sep='\\n')\n","\n","# Accuracy : one can use classification_report from sklearn\n","\n","print(\"\\nAccuracy of the model: {:.4f}%\\n\".format(accuracy_score(test_labels, prediction) * 100))\n","print(\"Classification report: \\n\", classification_report(test_labels, prediction, labels = [0, 1]), sep='\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Probabilities: \n","[[0.99276364]\n"," [0.01569769]\n"," [0.01924348]\n"," ...\n"," [0.5059785 ]\n"," [0.12448171]\n"," [0.99615866]]\n","\n","Labels:\n","[[1]\n"," [0]\n"," [0]\n"," ...\n"," [1]\n"," [0]\n"," [1]]\n","\n","Accuracy of the model: 86.3100%\n","\n","Classification report: \n","\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.87      0.86      5000\n","           1       0.87      0.85      0.86      5000\n","\n","    accuracy                           0.86     10000\n","   macro avg       0.86      0.86      0.86     10000\n","weighted avg       0.86      0.86      0.86     10000\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"TIICV-ySOYL0"},"source":["## Get predictions for random examples"]},{"cell_type":"code","metadata":{"id":"m2RmfNL3OYL0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631813498443,"user_tz":-330,"elapsed":382,"user":{"displayName":"Madiha Hanifa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWaIA9v7LU-MxIHfJ76VvMTHOYlE7ided8LBhwUw=s64","userId":"14011584313581478487"}},"outputId":"14dfb6f1-1a61-49b8-8a09-89748c517834"},"source":["# reviews on which we need to predict\n","sentence = [\"The movie was very touching and heart whelming\", \n","            \"I have never seen a terrible movie like this\", \n","            \"the movie plot is terrible but it had good acting\"]\n","\n","# convert to a sequence\n","sequences = tokenizer.texts_to_sequences(sentence)\n","\n","# pad the sequence\n","padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n","\n","# Get probabilities\n","print(\"Probablities : \")\n","print(model.predict(padded))\n","\n","# Get labels based on probability 1 if p>= 0.5 else 0\n","\n","for p in prediction:\n","    if p[0] >=0.5:\n","        p[0] = 1\n","    else:\n","        p[0] = 0\n","prediction = prediction.astype('int32') \n","print(\"\\nLabels:\", prediction, sep='\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Probablities : \n","[[0.9916369 ]\n"," [0.03113177]\n"," [0.06507811]]\n","\n","Labels:\n","[[1]\n"," [0]\n"," [0]\n"," ...\n"," [1]\n"," [0]\n"," [1]]\n"]}]}]}